---
title: "Project 2"
output: html_document
---
#Group member:Shivani, Luyao, Zijie
```{r}
library(ggplot2)
library(igraph)
library(tidyr)
library(dplyr)
library(reshape2)
library(forcats)
library(tidytext)
library(stringr)
library(ggraph)
library(tm)

```

```{r}
df <- read.csv('Keyword_data.csv', na.strings = "")
df_2017 <- read.csv('2017.csv')
df_2018 <- read.csv('2018.csv')
df_2019 <- read.csv('2019.csv')
df_2020 <- read.csv('2020.csv')
df_2021 <- read.csv('2021.csv')
df<- df %>% drop_na(Keyword.1)
```

# 1 Creating the Adjacency Matrix

> We have used 2 methods to create adjacency matrix, both give identical matrices.
> Method 1: Take pairwise combinationation of all Keyword columns and cross tabulate


# Construct Adjacency Matrix
```{r}
consolidated_df <- data.frame(matrix(ncol = 2, nrow = 0))
colnames(consolidated_df) <- c("Keyword1","Keyword2")
start_col <- match("Keyword.1",names(df))
end_col <- match("Keyword.12",names(df))


# Create consolidated dataframe with all keyword combinations that occur together

for(i in start_col:(end_col - 1)){
  for(j in (i+1):end_col)
  {
      temp <- df[,c(i,j)]
      colnames(temp) <- c("Keyword1","Keyword2")
      consolidated_df <- rbind(consolidated_df, temp)
  }
}
consolidated_df <- na.omit(consolidated_df)

# Take all unique keywords into consideration, to construct square matrix

levs <- unique(unlist(consolidated_df, use.names = FALSE))  

#Cross tabulate to form a matrix
adj_matrix <- table(lapply(consolidated_df, factor, levs))

#Add trasspose to make the matrix symmetric
adj_matrix <- adj_matrix + t(adj_matrix)
adj_matrix_df <- as.data.frame.matrix(adj_matrix)
adj_matrix <- as.matrix(adj_matrix_df)

adj_matrix
```

> Method 2: ***Works with original data file, that has "Title" column in it***
> Make a pivot longer form from all keywords and cross tabulate for all Titles and Keywords.
> Suppose we have n documents and m keywords, this step creates an n x m matrix, which indicates the occurance
> of a keyword in each Title.
> Matrix multiple transpose of this matrix with itself (m x n matrix multiply n x m), which results in m x m
> matrix. Set diagonal to 0 to completely form adjacency matrix

```{r}
Keyword <- read.csv('Keyword_data.csv',na="")
keyword_sub <- Keyword %>% drop_na(Keyword.1)
keymatrix <- keyword_sub %>%
pivot_longer(-Title, names_to = "Category", values_to = "Keyword") %>%
xtabs(~Title + Keyword, data = ., sparse = FALSE) %>% 
crossprod(., .) 
diag(keymatrix ) <- 0
keymatrix
```
> Method 3: Code provided by Professor (Data file required does not contain "Title" column, only 49 rows with Keyword columns)

```{r}
Keyword_data <- read.csv('Keyword_data_new.csv', na = "")
# Stack all variables to find unique
s<-stack(Keyword_data)
# Calculate unique keywords
u<-na.omit(unique(s$values))
# Create a weighted adjacency matrix
answer<-matrix(0, nrow=length(u), ncol=length(u))
colnames(answer)<-u
rownames(answer)<-u
# Logic to create weighted matrix
for(i in 1:length(Keyword_data$Keyword.2)){
  temp<-unlist(Keyword_data[i,])
  temp<-temp[!is.na(temp)]
  keyword_list<-combn(temp,2)
  for(j in 1:length(keyword_list[1,])){
    rowind<-which(rownames(answer)==(keyword_list[1,j]))
    colind<-which(colnames(answer)==(keyword_list[2,j]))
    answer[rowind,colind]<-answer[rowind,colind]+1
    answer[colind,rowind]<-answer[colind,rowind]+1
  }
}

```


# 2 Create graph from adjacency matrix and plot

```{r}
net1<-graph_from_adjacency_matrix(answer,mode="undirected", weighted = TRUE)
net1
plot.igraph(net1)
```

# 3 Computing Node degrees

```{r}
deg <- degree(net1, mode="all")
data.frame(deg)
```

# 3 Computing Node Strength

```{r}
strength <- strength(net1, mode="all")
data.frame(strength)
```

# 4 Top 10 nodes by degree

```{r}
sort(deg, decreasing = TRUE)[1:10]
```

# 4 Top 10 nodes by strength

```{r}
sort(strength, decreasing = TRUE)[1:10]
```

# 5 Top 10 node pairs by weight

```{r}
edge_list <- get.data.frame(net1)
top_edge_list <- edge_list %>% 
                  arrange(desc(weight))
top_edge_list[1:10,]

#V(net1)$name[strength(net1)==max(strength(net1))]
```
# 6 Plotting Average strength vs degree
```{r}
# Create a dataframe with node degrees and strength

deg_strength <- data.frame(cbind(deg, strength))
deg_strength <- deg_strength %>% arrange(deg)

# Group by degree and calulate average strength for each degree

avg_strength_deg <- deg_strength %>%
        group_by(deg) %>%
        summarise(avg_strength = mean(strength))
avg_strength_deg

# plot average strength vs degree

ggplot(avg_strength_deg, aes(x=deg, y=avg_strength)) + 
  geom_point()+
  geom_smooth(method=lm)
```

# Task 2

```{r}

# Filter tweet column from all dataframes and create a single dataframe

tweets_2017<- data.frame(df_2017$tweet)
tweets_2018 <- data.frame(df_2018$tweet)
tweets_2019 <- data.frame(df_2019$tweet)
tweets_2020 <- data.frame(df_2020$tweet)
tweets_2021 <- data.frame(df_2021$tweet)
colnames(tweets_2017)[colnames(tweets_2017) == 'df_2017.tweet'] <- 'tweet'
colnames(tweets_2018)[colnames(tweets_2018) == 'df_2018.tweet'] <- 'tweet'
colnames(tweets_2019)[colnames(tweets_2019) == 'df_2019.tweet'] <- 'tweet'
colnames(tweets_2020)[colnames(tweets_2020) == 'df_2020.tweet'] <- 'tweet'
colnames(tweets_2021)[colnames(tweets_2021) == 'df_2021.tweet'] <- 'tweet'

tweets_2017$year <- 2017
tweets_2018$year <- 2018
tweets_2019$year <- 2019
tweets_2020$year <- 2020
tweets_2021$year <- 2021
all_tweets <- do.call("rbind", list(tweets_2017, tweets_2018, tweets_2019, tweets_2020, tweets_2021))
```

> Data preprocessing. We remove things like special symbols (&, <, >), URLs, emojis, stopwords, stopwords without apostrophe (eg dont), usernames. Not removing numbers because tweets mention things like "model 3" and "falcon 9" very frequently

```{r}
tweet_words <- all_tweets %>%
            mutate(tweet = str_remove_all(tweet, "&amp;|&lt;|&gt;"),   
            tweet = str_remove_all(tweet, "\\s?(f|ht)(tp)(s?)(://)([^\\.]*)[\\.|/](\\S*)"),
            tweet = str_remove_all(tweet, "[^\x01-\x7F]")    
            ) %>%  
            unnest_tweets(word, tweet, strip_punct = TRUE) %>%
            filter(!word %in% stop_words$word,
                   !word %in% str_remove_all(stop_words$word, "'"),  
                  # str_detect(word, "[a-z]"), 
                   !str_detect(word, "@\\S+") 
                   ) %>%
            count(year, word, sort=TRUE)

# Get total number of words per year

total_words <- tweet_words %>% 
            group_by(year) %>% 
            summarize(total = sum(n))

tweet_words <- left_join(tweet_words, total_words)

# Calculate word frequency

tweet_words <- tweet_words %>%
            mutate(freq = n/total) %>%
            arrange(desc(freq))

# Function to get top words by year
get_top <- function(tweet_words, yy, num){
  temp <- tweet_words %>%
            filter(year == yy) %>%
            slice_max(order_by = freq, n = num)
  return(temp)
}

```

# 2 Show top words by frequency per year 

```{r}
top_words_2017 <- get_top(tweet_words, 2017, 10)
top_words_2018 <-get_top(tweet_words, 2018, 10)
top_words_2019 <-get_top(tweet_words, 2019, 10)
top_words_2020 <-get_top(tweet_words, 2020, 10)
top_words_2021 <-get_top(tweet_words, 2021, 10)

# Display top words

top_words_2017
top_words_2018
top_words_2019
top_words_2020
top_words_2021
```

# 3 Plot histogram of word frequencies per year

```{r}
ggplot(tweet_words, aes(freq, fill = year)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.0009) +  #check
  facet_wrap(~year, ncol = 2, scales = "free_y")
```

# 4 Zipf's law

```{r}
freq_by_rank <- tweet_words %>% 
  group_by(year) %>% 
  mutate(rank = row_number()) %>%
  ungroup()

freq_by_rank %>% 
  ggplot(aes(rank, freq, color = year)) + 
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()

```

> Bigram network: Function to create a bigram network based on year and threshold

```{r}
get_bigram_network <- function(yr, threshold){
  
tweet_bigrams <- all_tweets %>% 
                 filter(year==yr) %>%
                 mutate(tweet = str_remove_all(tweet, "&amp;|&lt;|&gt;"),   
                        tweet = str_remove_all(tweet, "\\s?(f|ht)(tp)(s?)(://)([^\\.]*)[\\.|/](\\S*)"), 
                        tweet = str_remove_all(tweet, "[^\x01-\x7F]")    
                        ) %>% 
                 unnest_tokens(bigram, tweet, token = "ngrams", n = 2)


# bigrams with stop words
bigrams_separated <- tweet_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) 
#!word %in% str_remove_all(stop_words$word, "'")

bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE) %>%
  drop_na()

bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

bigram_graph <- bigram_counts %>%
  filter(n > threshold) %>%
  graph_from_data_frame()

return(bigram_graph)
}
```

```{r}
# Create bigram networks for yearly tweets, threshold is to set words with minimum count to be
# included in the diagram
bigram_graph_2017 <- get_bigram_network(2017, 4)
bigram_graph_2018 <- get_bigram_network(2018, 5)
bigram_graph_2019 <- get_bigram_network(2019, 10)
bigram_graph_2020 <- get_bigram_network(2020, 11)
bigram_graph_2021 <- get_bigram_network(2021, 11)

a <- grid::arrow(type = "closed", length = unit(.1, "inches"))

# Plot all networks

ggraph(bigram_graph_2017, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.1, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1.5, hjust = 0.7) +
  theme_void()

ggraph(bigram_graph_2018, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.1, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1.5, hjust = 0.7) +
  theme_void()

ggraph(bigram_graph_2019, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.1, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1.5, hjust = 0.7) +
  theme_void()

ggraph(bigram_graph_2020, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.1, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1.5, hjust = 1) +
  theme_void()

ggraph(bigram_graph_2021, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.1, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1.5, hjust = 0.7) +
  theme_void()
```

















