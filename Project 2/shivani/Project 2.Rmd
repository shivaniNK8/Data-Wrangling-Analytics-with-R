---
title: "Project 2"
output: html_document
---
```{r}
library(ggplot2)
library(igraph)
library(tidyr)
library(dplyr)
library(reshape2)
library(forcats)

```

```{r}
try <- read.csv('try.csv')
df <- read.csv('Keyword_data.csv', na.strings = "")
df_2017 <- read.csv('2017.csv')
df_2018 <- read.csv('2018.csv')
df_2019 <- read.csv('2019.csv')
df_2020 <- read.csv('2020.csv')
df_2021 <- read.csv('2021.csv')
df_sub <- df[1:7,1:3]
df_sub <- df %>% drop_na(Keyword.1)
```

```{r}
df1 <- df_sub %>%
pivot_longer(-Title, names_to = "Category", values_to = "Keyword") %>%
xtabs(~Title + Keyword, data = ., sparse = FALSE) %>% 
crossprod(., .) 
diag(df1 ) <- 0
df1
```



# Construct Adjacency Matrix
```{r}
consolidated_df <- data.frame(matrix(ncol = 2, nrow = 0))
colnames(consolidated_df) <- c("Keyword1","Keyword2")
start_col <- 2
end_col <- 13
for(i in start_col:(end_col - 1)){
  for(j in (i+1):end_col)
  {
      #print(paste0(i,',',j))
      temp <- df[,c(i,j)]
      colnames(temp) <- c("Keyword1","Keyword2")
      consolidated_df <- rbind(consolidated_df, temp)
  }
}
consolidated_df <- na.omit(consolidated_df)
levs <- unique(unlist(consolidated_df, use.names = FALSE))
adj_matrix <- table(lapply(consolidated_df, factor, levs))
adj_matrix <- adj_matrix + t(adj_matrix)
adj_matrix_df <- as.data.frame.matrix(adj_matrix)
adj2 <- as.matrix(adj_matrix_df)

adj2_n <- adj2[order(rownames(adj2)), order(colnames(adj2))]


# adj_matrix <- as.data.frame.matrix (df1 %>%
#   xtabs(~Keyword2 + Keyword1, data=.))
# 
# as.data.frame.matrix (table(df1$Keyword1, df1$Keyword2))
# class(as.matrix(adj_matrix))
```

```{r}
net1<-graph_from_adjacency_matrix(adj_matrix,mode="undirected", weighted = TRUE)
net1
plot.igraph(net1)
#tkplot(net1)
```
#Node degrees
```{r}
deg <- degree(net1, mode="all")
data.frame(sort(deg))
```
# Node Strength
```{r}
strength <- strength(net1, mode="all")
data.frame(strength)
```
#Top 10 nodes by degree
```{r}
sort(deg, decreasing = TRUE)[1:10]
```
```{r}
sort(strength, decreasing = TRUE)[1:10]
```


> Top 10 node pairs by weight

```{r}
edge_list <- get.data.frame(net1)
top_edge_list <- edge_list %>% 
                  arrange(desc(weight))
top_edge_list[1:10,]

V(net1)$name[strength(net1)==max(strength(net1))]
```
# Average strength vs degree
```{r}

deg_strength <- data.frame(cbind(deg, strength))
deg_strength <- deg_strength %>% arrange(deg)
avg_strength_deg <- deg_strength %>%
        group_by(deg) %>%
        summarise(avg_strength = mean(strength))
avg_strength_deg
#plot(avg_strength_deg$deg, avg_strength_deg$avg_strength)

ggplot(avg_strength_deg, aes(x=deg, y=avg_strength)) + 
  geom_point()+
  geom_smooth(method=lm)
```


```{r}
as_edgelist(net1)
as_adjacency_matrix(net1)
plot(net1, layout = layout.kamada.kawai,
     vertex.color="blue", vertex.size=7,
     vertex.frame.color="gray", vertex.label.color="black",
     vertex.label.cex=0.5, vertex.label.dist=0.5,
     edge.curved=0.2, edge.arrow.size=0.4,
     edge.color="light blue")
```

# Task 2

```{r}
snowball_stopwords <- stop_words %>% 
                    filter(lexicon == 'snowball')
tweets_2017<- data.frame(df_2017$tweet)
tweets_2018 <- data.frame(df_2018$tweet)
tweets_2019 <- data.frame(df_2019$tweet)
tweets_2020 <- data.frame(df_2020$tweet)
tweets_2021 <- data.frame(df_2021$tweet)
colnames(tweets_2017)[colnames(tweets_2017) == 'df_2017.tweet'] <- 'tweet'
colnames(tweets_2018)[colnames(tweets_2018) == 'df_2018.tweet'] <- 'tweet'
colnames(tweets_2019)[colnames(tweets_2019) == 'df_2019.tweet'] <- 'tweet'
colnames(tweets_2020)[colnames(tweets_2020) == 'df_2020.tweet'] <- 'tweet'
colnames(tweets_2021)[colnames(tweets_2021) == 'df_2021.tweet'] <- 'tweet'

tweets_2017$year <- 2017
tweets_2018$year <- 2018
tweets_2019$year <- 2019
tweets_2020$year <- 2020
tweets_2021$year <- 2021
all_tweets <- do.call("rbind", list(tweets_2017, tweets_2018, tweets_2019, tweets_2020, tweets_2021))
```

```{r}
#Remove &, <, >, URLs, emojis, numbers, stopwords, stopwords without apostrophe (ed dont)
tweet_words <- all_tweets %>%
            #mutate(tweet = str_remove_all(tweet, "&amp;|&lt;|&gt;")) %>%
            mutate(tweet = str_remove_all(tweet, "&amp;|&lt;|&gt;"),    #Remove &, <,  >
            tweet = str_remove_all(tweet, "\\s?(f|ht)(tp)(s?)(://)([^\\.]*)[\\.|/](\\S*)"),  #remove URLS
            tweet = str_remove_all(tweet, "[^\x01-\x7F]")     #remove emojis
            ) %>%  
            unnest_tweets(word, tweet, strip_punct = TRUE) %>%
                #unnest_tokens(word, tweet, token = "tweets") %>%
            filter(!word %in% stop_words$word,
                   !word %in% str_remove_all(stop_words$word, "'"),  #remove stopwords that don't have apostrophe
                  # str_detect(word, "[a-z]"), #remove numbers
                   #!str_detect(word, "@\\S+")  #remove usernames?
                   ) %>%
            count(year, word, sort=TRUE)

total_words <- tweet_words %>% 
            group_by(year) %>% 
            summarize(total = sum(n))

tweet_words <- left_join(tweet_words, total_words)
tweet_words <- tweet_words %>%
            mutate(freq = n/total) %>%
            arrange(desc(freq))

get_top <- function(tweet_words, yy, num){
  temp <- tweet_words %>%
            filter(year == yy) %>%
            slice_max(order_by = freq, n = num)
  return(temp)
}

top_words_2017 <- get_top(tweet_words, 2017, 10)
top_words_2018 <-get_top(tweet_words, 2018, 10)
top_words_2019 <-get_top(tweet_words, 2019, 10)
top_words_2020 <-get_top(tweet_words, 2020, 10)
top_words_2021 <-get_top(tweet_words, 2021, 10)

top_words_2017
top_words_2018
top_words_2019
top_words_2020
top_words_2021

```
```{r}
ggplot(tweet_words, aes(freq, fill = year)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.0009) +  #check
  facet_wrap(~year, ncol = 2, scales = "free_y")
```
> Zipf's law

```{r}
freq_by_rank <- tweet_words %>% 
  group_by(year) %>% 
  mutate(rank = row_number()) %>%
  ungroup()

freq_by_rank %>% 
  ggplot(aes(rank, freq, color = year)) + 
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()

```
> Bigram network

```{r}

tweet_bigrams <- all_tweets %>% 
                filter(year==2017) %>%
                  mutate(tweet = str_remove_all(tweet, "&amp;|&lt;|&gt;"),    #Remove &, <,  >
                      tweet = str_remove_all(tweet, "\\s?(f|ht)(tp)(s?)(://)([^\\.]*)[\\.|/](\\S*)"),  #remove URLS
                     tweet = str_remove_all(tweet, "[^\x01-\x7F]")     #remove emojis
                    ) %>% 
                  unnest_tokens(bigram, tweet, token = "ngrams", n = 2)

tweet_bigrams %>%
  count(bigram, sort = TRUE)

# bigrams with stop words
bigrams_separated <- tweet_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) 
#!word %in% str_remove_all(stop_words$word, "'")

bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE) %>%
  drop_na()

bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

bigram_counts

```
```{r}
bigram_graph <- bigram_counts %>%
  filter(n > 4) %>%
  graph_from_data_frame()

a <- grid::arrow(type = "closed", length = unit(.1, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.1, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1.5, hjust = 0.7) +
  theme_void()
```

















